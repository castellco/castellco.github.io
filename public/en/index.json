


[{"content":"","date":"31 December 2025","externalUrl":null,"permalink":"/tags/f%C3%BAtbol/","section":"Tags","summary":"","title":"Fútbol","type":"tags"},{"content":"","date":"31 December 2025","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"31 December 2025","externalUrl":null,"permalink":"/tags/web-scraping/","section":"Tags","summary":"","title":"Web Scraping","type":"tags"},{"content":"\rEn este post explicaré cómo construí un scraper en Python para recolectar las estadísticas de los jugadores de la Euro 2024 desde la web oficial de la UEFA.\n¿Por qué?\r#\rEntre junio y julio del 2024 tuvo lugar la EURO 2024. En ese contexto, participé de un reto del chapter local de Omdena en Tunisia: UEFA EURO 2024 - Leveraging Machine Learning and Open Data Sets for Advanced Sports Analytics. El propósito era analizar este evento en lo deportivo y lo económico. Si bien nos quedamos cortos en este último aspecto, fue un espacio interesante para aplicar técnicas de ciencia de datos al mundo del fútbol.\nAsí, varios colegas se dispusieron a buscar datasets ya existentes: estadísticas de cada partido, datasets históricos o lo que haya. Cuando visité la página de la EURO, vi que era posible scrapearla para generar un dataset.\nA modo de documentación de ese ejercicio, en este post resumo la lógica detrás del scraper en Python + Selenium que hice y que extrae las estadísticas de jugadores desde la web de la UEFA y las prepara en un pandas.DataFrame para análisis exploratorio y modelado.\nResumen técnico\r#\rEl objetivo principal es automatizar la recolección de datos de los jugadores de la EURO a falta de acceso a una API oficial. El output debía ser un dataset en formato tabular.\nUsé Python por ser el lenguaje de preferencia en los proyectos de Omdena en los que he participado, aunque también R hubiera servido. La librería principal fue Selenium, que permite controlar el navegador web (en mi caso, Chrome) para interactuar con páginas dinámicas que cargan contenido vía JavaScript.\nHacia los meses que hice este scraper, los términos y condiciones de la web de la UEFA permitían el web scraping para fines no comerciales. Recomiendo revisar los términos actuales antes de ejecutar cualquier scraper.\nPasos\r#\rEl scraper, que es open source, está disponible en GitHub:\ncastellco/euro_2024_scraper\rGetting the game statistics of EURO 2024 players with Python and Selenium\rJupyter Notebook\r0\r0\rLas condiciones previas son tener conexión a internet, Python 3.8+ instalado y las librerías necesarias (ver requirements.txt en el repo).\nBásicamente, los pasos para ejecutarlo son:\nPreparar del entorno\r#\rPrimero se importan las librerías necesarias e instala el driver de Chrome automáticamente con webdriver_manager. Luego, se configuran las opciones del navegador Chrome (p. ej. --start-maximized). Esto es para que se ejecute Chrome en modo gráfico y no headless —ya que algunas tablas no cargan bien en modo headless— y también por una preferencia personal: prefiero ver lo que hace el scraper en tiempo real.\n# 1: import libraries ----------------------------------------------------- import time from selenium import webdriver from selenium.webdriver.chrome.service import Service from webdriver_manager.chrome import ChromeDriverManager from selenium.webdriver.common.by import By from selenium.webdriver import ActionChains import pandas as pd import re chrome_options = webdriver.ChromeOptions() chrome_options.add_argument(\u0026#34;--start-maximized\u0026#34;)\rAbrir la web y preparar la página\r#\rPosteriormente, se instala e inicia el webdriver.Chrome(...) y se carga la página https://www.uefa.com/european-qualifiers/statistics/players/.\n# 2: enter website, reject cookies and define function to scroll ---------- driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options) driver.get(\u0026#34;https://www.uefa.com/european-qualifiers/statistics/players/\u0026#34;)\rEsperamos 2 segundos. Como se podrá apreciar, a lo largo del script usé varias pausas con time.sleep() para dar tiempo a que la página cargue dinámicamente y para, en teoría, evitar bloqueos por parte del servidor web. Luego, para indicarle a Selenium qué lugar de la página clickear para rechazar las cookies, uso un CSS selector que apunta al botón de cookies ya que no me fue posible identificar el XPath.\nEn este punto, es importante comentar que para identificar CSS selectors, la extensión de Chrome SelectorGadget siempre me ha parecido excelente.\nRechazadas ya las cookies, definí la función scroll_to_sponsors() para desplazar la página hacia abajo y con ello forzar la carga completa de la tabla dinámica de jugadores. En específico, esta función busca el banner de sponsors (que en ese entonces estaba al final de la página) usando su XPath y desplaza la vista hasta ese elemento. Esto es necesario porque la tabla de jugadores carga más filas a medida que se hace scroll hacia abajo. Necesitaba tener toda la tabla cargada.\n# define function to scroll down, in order to make rest of the table discoverable def scroll_to_sponsors(): try: sponsors_xpath = \u0026#34;//div[contains(text(), \u0026#39;Official global sponsors\u0026#39;)]\u0026#34; select_sponsors_banner = driver.find_element(By.XPATH, sponsors_xpath) ActionChains(driver)\\ .scroll_to_element(select_sponsors_banner)\\ .perform() time.sleep(2) except: try: sponsors_xpath = \u0026#39;//div[@class=\u0026#34;pk-container pk-bg--background lazyloaded\u0026#34; and @role=\u0026#34;region\u0026#34; and @pk-theme=\u0026#34;light\u0026#34; and @aria-label=\u0026#34;\u0026#34;]\u0026#39; select_sponsors_banner = driver.find_element(By.XPATH, sponsors_xpath) ActionChains(driver)\\ .scroll_to_element(select_sponsors_banner)\\ .perform() time.sleep(2) except: print(\u0026#34;The scroll_to_sponsors function didn\u0026#39;t work.\u0026#34;) time.sleep(2)\rTambién creé un dataset vacío con pandas.DataFrame() y varias listas vacías para almacenar los datos de cada jugador mientras se itera sobre ellos.\n# will need these later: dataset = pd.DataFrame() name, national_team, club, overview_figures, overview_labels, stats_figures, stats_labels = [], [], [], [], [], [], []\rDefinir funciones auxiliares de extracción\r#\rLuego, definí varias funciones auxiliares para extraer datos específicos de la página. Esta parte fue la que, de lejos, me demandó más tiempo y esfuerzo. Fue prueba y error. Recozco que hay muchas cosas mejorables, pero el objetivo era tener un scraper funcional en un tiempo razonable.\nLa primera función, extract_name_and_teams(), extrae el nombre completo del jugador, su selección nacional y su club actual. Usé varios try/except para manejar casos donde algún dato no esté disponible o el selector falle.Nuevamente usé CSS selectors y XPath para localizar los elementos gracias a la extensión SelectorGadget.\nLa función extract_list_with_xpath(labels_xpath) toma un XPath como argumento y devuelve una lista de textos de los elementos que coinciden con ese XPath. Esto se usa para extraer tanto las etiquetas como las cifras de las secciones de \u0026ldquo;overview\u0026rdquo; y \u0026ldquo;statistics\u0026rdquo;.\nopen_accordions es una función que abre las secciones colapsables (acordeones) en la pestaña de estadísticas del jugador para asegurarse de que todos los datos estén visibles y accesibles para la extracción.\nFinalmente, update_dataset(dataset) toma el dataset actual y agrega una nueva fila con los datos del jugador actual. Esta función maneja la normalización de columnas entre distintos jugadores, evitando duplicados y asegurando que todas las filas tengan las mismas columnas, incluso si algunos jugadores no tienen ciertos datos. Esta función es mejorable por los tantos if/elif anidados, pero en el momento sirvió para el propósito. Un punto de mejora es simplificar los casos en los que ciertos jugadores tienen más o menos columnas que otros.\n# 3: define main functions ------------------------------------------------ def extract_name_and_teams(): try: name = driver.find_element(By.CSS_SELECTOR, \u0026#39;.player-header__name--first\u0026#39;).text + \u0026#39; \u0026#39; + driver.find_element(By.CSS_SELECTOR, \u0026#39;.player-header__name--last\u0026#39;).text print(name) except: print(\u0026#39;Player name and/or last name not found.\u0026#39;) try: # national_team = driver.find_element(By.CSS_SELECTOR, \u0026#39;.player-header__teams \u0026gt; div:nth-child(1) \u0026gt; a:nth-child(3) \u0026gt; pk-identifier:nth-child(1) \u0026gt; div:nth-child(2) \u0026gt; span:nth-child(1)\u0026#39;).text # print(national_team) national_team = driver.find_element(By.XPATH, \u0026#39;//span[@class=\u0026#34;player-header__team-name pk-text--text-01\u0026#34;][1]\u0026#39;).text print(national_team) except: print(\u0026#39;National team not found.\u0026#39;) try: club = driver.find_element(By.CSS_SELECTOR, \u0026#39;.player-header__teams \u0026gt; div:nth-child(2) \u0026gt; a:nth-child(3) \u0026gt; pk-identifier:nth-child(1) \u0026gt; div:nth-child(2) \u0026gt; span:nth-child(1)\u0026#39;).text print(club) except: try: print(\u0026#39;Club not found. Trying another xpath...\u0026#39;) club = driver.find_element(By.XPATH, \u0026#39;/html/body/div[3]/div/div/div[2]/div[3]/div[2]/div[2]/pk-identifier/div/div[1]/div[2]/pk-identifier/div/span\u0026#39;).text print(club) except: print(\u0026#39;Club not found.\u0026#39;) club = \u0026#34;\u0026#34; return name, national_team, club def extract_list_with_xpath(labels_xpath): if driver.find_elements(By.XPATH, \u0026#34;//h2[contains(text(), \u0026#39;Qualifying stats\u0026#39;)]\u0026#34;): labels_text = [] print(\u0026#39;Player only participated in qualifyings.\u0026#39;) else: all_labels = driver.find_elements(By.XPATH, labels_xpath) labels_text = [label.text for label in all_labels] return labels_text def open_accordions(): for i in range(0, 6): try: driver.find_element(By.CSS_SELECTOR, \u0026#39;#accordion-item-\u0026#39; + str(i) + \u0026#39; \u0026gt; pk-accordion-item-title:nth-child(1) \u0026gt; h2:nth-child(1)\u0026#39;).click() time.sleep(2) except: print(\u0026#39;There are no more accordions to open.\u0026#39;) def update_dataset(dataset): player_info = [name, national_team, club] + overview_figures if stats_figures: player_info += stats_figures else: player_info += [\u0026#34;\u0026#34;] * len(stats_figures) if stats_labels: columns = [\u0026#34;name\u0026#34;, \u0026#34;national_team\u0026#34;, \u0026#34;club\u0026#34;] + overview_labels + stats_labels else: columns = [\u0026#34;name\u0026#34;, \u0026#34;national_team\u0026#34;, \u0026#34;club\u0026#34;] + overview_labels new_row = pd.DataFrame([player_info], columns=columns) new_row = new_row.loc[:,~new_row.columns.duplicated()].copy() dataset = dataset.loc[:,~dataset.columns.duplicated()].copy() if dataset.empty: print(\u0026#34;Condition met: dataset.empty\u0026#34;) elif len(dataset.columns) == len(columns): print(\u0026#34;Condition met: len(dataset.columns) == len(columns)\u0026#34;) try: dataset = dataset[columns] except: print(\u0026#39;Something went wrong when executing the if statement of the len(dataset.columns) == len(columns) condition in case of \u0026#39; + name + \u0026#39; from \u0026#39; + national_team) elif len(dataset.columns) \u0026lt; len(columns): print(\u0026#34;Condition met: len(dataset.columns) \u0026lt; len(columns)\u0026#34;) try: for column in columns: if column not in dataset.columns: dataset[column] = \u0026#34;\u0026#34; else: continue dataset = dataset[new_row.columns] except: print(\u0026#39;Something went wrong when executing the if statement of the len(dataset.columns) \u0026lt; len(columns) condition in case of \u0026#39; + name + \u0026#39; from \u0026#39; + national_team) elif len(dataset.columns) \u0026gt; len(columns): print(\u0026#34;Condition met: len(dataset.columns) \u0026gt; len(columns)\u0026#34;) try: for column in dataset.columns: if column not in columns: new_row[column] = \u0026#34;\u0026#34; else: continue new_row = new_row[dataset.columns] except: print(\u0026#39;Something went wrong when executing the if statement of the len(dataset.columns) \u0026gt; len(columns) condition in case of \u0026#39; + name + \u0026#39; from \u0026#39; + national_team) else: print(\u0026#39;Something went wrong when executing update_dataset function.\u0026#39;) dataset = pd.concat([dataset, new_row], ignore_index=True) return dataset\rEjecución del scraper\r#\rAhora, se ejecuta el scraper. Este es el paso que demora más, ya que hay muchos países y jugadores, y por los tiempos de espera que añadí por las razones que comenté párrafos arriba.\nLa lógica es iterar sobre cada país participante (del 2 al 54 en el acordeón de selección de países) y luego sobre cada jugador listado para ese país. Para próximaas ediciones, el range podría cambiar.\nPor cada jugador, se extraen los datos usando las funciones definidas previamente y se actualiza el dataset.\n# 4: scraping work: iterate over each country and each player ------------- for i in range(2, 55): # open main site driver.get(\u0026#34;https://www.uefa.com/european-qualifiers/statistics/players/\u0026#34;) # select main tournament (excluding qualiying) time.sleep(2) try: main_tournament = driver.find_element(By.XPATH, \u0026#39;//pk-accordion-item[1]/pk-accordion-item-content/pk-radio/pk-radio-option[1]\u0026#39;) time.sleep(2) main_tournament.click() time.sleep(2) except: main_tournament = driver.find_element(By.XPATH, \u0026#39;//input[@class=\u0026#34;pk-radio\u0026#34; and @name=\u0026#34;phase\u0026#34; and @title=\u0026#34;phase\u0026#34; and @type=\u0026#34;radio\u0026#34; and @id=\u0026#34;tournament\u0026#34; and @value=\u0026#34;TOURNAMENT\u0026#34; and @part=\u0026#34;input\u0026#34;]\u0026#39;) time.sleep(2) main_tournament.click() time.sleep(2) # select country xpath_country = \u0026#39;//pk-accordion-item[2]/pk-accordion-item-content/div/pk-radio/pk-radio-option[\u0026#39; + str(i) + \u0026#39;]/span\u0026#39; select_country = driver.find_element(By.XPATH, xpath_country) country_name = select_country.text print(\u0026#39;---------- Accessing info of \u0026#39; + country_name + \u0026#39; ----------\u0026#39;) time.sleep(2) select_country.click() # select country time.sleep(3) # scroll down twice, as needed to discover the whole page scroll_to_sponsors() scroll_to_sponsors() # gather all players\u0026#39; stats website links try: player_xpath = \u0026#39;//a[contains(@class, \u0026#34;pk-w--100\u0026#34;) and contains(@href, \u0026#34;/api/v1/linkrules/player/\u0026#34;) and contains(@href, \u0026#34;/statistics?competitionId=3\u0026amp;phase=TOURNAMENT\u0026#34;)]\u0026#39; select_player = driver.find_elements(By.XPATH, player_xpath) except: print(\u0026#39;Seems like the site of \u0026#39; + country_name + \u0026#39; is empty.\u0026#39;) continue players_ids = [] for link in select_player: href = link.get_attribute(\u0026#39;href\u0026#39;) player_id = re.search(r\u0026#39;player/(\\d+)/\u0026#39;, href).group(1) players_ids.append(player_id) for player_id in players_ids: time.sleep(2) driver.get(\u0026#39;https://www.uefa.com/euro2024/teams/players/\u0026#39; + player_id + \u0026#39;/\u0026#39;) print(\u0026#39;---------- Working on player whose ID is \u0026#39; + player_id + \u0026#39; -------------\u0026#39;) time.sleep(2) name, national_team, club = extract_name_and_teams() overview_labels = extract_list_with_xpath(\u0026#39;//span[@class=\u0026#34;player-profile-category\u0026#34;]\u0026#39;) overview_figures = extract_list_with_xpath(\u0026#39;//span[@class=\u0026#34;player-profile-value\u0026#34;]\u0026#39;) print(overview_labels) print(overview_figures) time.sleep(2) driver.get(\u0026#39;https://www.uefa.com/euro2024/teams/players/\u0026#39; + player_id + \u0026#39;/statistics/\u0026#39;) time.sleep(2) scroll_to_sponsors() open_accordions() stats_labels = extract_list_with_xpath(\u0026#39;//div[@slot=\u0026#34;stat-label\u0026#34;]\u0026#39;) stats_figures = extract_list_with_xpath(\u0026#39;//div[@slot=\u0026#34;stat-value\u0026#34;]\u0026#39;) print(stats_labels) print(stats_figures) dataset = update_dataset(dataset) print(\u0026#39;---------- End of process for player whose ID is \u0026#39; + player_id + \u0026#39; ----------\u0026#39;) time.sleep(2)\rAl final esta sección, se obtiene un dataset (en mi caso, una tabla de 621x66).\nEl dataset publicado está en el repo en Dagshub.\n","date":"31 December 2025","externalUrl":null,"permalink":"/posts/euro-2024-scraper/","section":"Posts","summary":"Aquí repaso cómo hice un script de web scraping en Python para recolectar las estadísticas de los jugadores de la Euro 2024.","title":"Web Scraping con Python: La Euro 2024","type":"posts"},{"content":"","date":"26 December 2025","externalUrl":null,"permalink":"/en/","section":"","summary":"","title":"","type":"page"},{"content":"You can also take a look at my LinkedIn profile: https://www.linkedin.com/in/cornejocastellano\nExperience\rOrganization\rLink\rTitle\rDates\rLocation\rVestas\rConstruction Quality Deployment Lead\r2025 – Present\rMadrid, Spain\rWorld Vision\rNational Information Management Analyst\r2024 – 2025\rLima, Peru\rVestas\rData Management Trainee\r2023\rMadrid, Spain\rEconomic and Social Research Consortium\rMonitoring and Evaluation Database Manager\r2019 – 2021\rLima, Peru\rPontifical Catholic University of Peru\rResearch Assistant\r2018 – 2021\rLima, Peru\rFondazione l'Albero della Vita\rIntern\r2018\rLima, Peru\rEducation\rInstitution\rLink\rDegree\rDates\rCarlos III University of Madrid\rMaster's in Computational Social Science\r2024\rPontifical Catholic University of Peru\rDiploma in Data Science for Social Sciences and Public Management\r2021-2022\rESAN University\rDiploma in Monitoring and Evaluation of Programs and Projects\r2024\rPontifical Catholic University of Peru\rBachelor in Communication for Development\r2021-2022\rPublications\rCover\rTitle\rReference\rPublisher\rExploring terrorism in Peru: a spatial approach\rCornejo Castellano, C. (2023)\rCarlos III University of Madrid\rGuide for conducting journalistic investigations on human trafficking (editorial supervision)\rFernández, L. (2021)\rEconomic and Social Research Consortium (CIES)\rGender-based violence in the family environment against children and adolescents perceived as part of the LGTBI population\rHuaita Alegre, M., Chavez Granda, J., Cornejo Castellano, G. \u0026 Saravia Pinazo, M. (2019)\rJudicial Branch of Peru\rThe ratification of CEDAW as a milestone in the struggle for women's rights in Peru\rHuaita Alegre, M., \u0026 Cornejo Castellano, G. (2019).\rIus et Veritas\rEquality to build democracy: Analysis of LGTBI candidacies in the 2006–2016 electoral processes\rAlza, Carlos, et al. (2017).\rNational Jury of Elections and Observatory of Public Policies for Sexual Diversity (DISEX), PUCP\r","date":"26 December 2025","externalUrl":null,"permalink":"/en/cv/","section":"","summary":"You can also take a look at my LinkedIn profile: https://www.linkedin.com/in/cornejocastellano\nExperience\rOrganization\rLink\rTitle\rDates\rLocation\rVestas\rConstruction Quality Deployment Lead\r2025 – Present\rMadrid, Spain\rWorld Vision\rNational Information Management Analyst\r2024 – 2025\rLima, Peru\rVestas\rData Management Trainee\r2023\rMadrid, Spain\rEconomic and Social Research Consortium\rMonitoring and Evaluation Database Manager\r2019 – 2021\rLima, Peru\rPontifical Catholic University of Peru\rResearch Assistant\r2018 – 2021\rLima, Peru\rFondazione l'Albero della Vita\rIntern\r2018\rLima, Peru\rEducation\rInstitution\rLink\rDegree\rDates\rCarlos III University of Madrid\rMaster's in Computational Social Science\r2024\rPontifical Catholic University of Peru\rDiploma in Data Science for Social Sciences and Public Management\r2021-2022\rESAN University\rDiploma in Monitoring and Evaluation of Programs and Projects\r2024\rPontifical Catholic University of Peru\rBachelor in Communication for Development\r2021-2022\rPublications\rCover\rTitle\rReference\rPublisher\rExploring terrorism in Peru: a spatial approach\rCornejo Castellano, C. (2023)\rCarlos III University of Madrid\rGuide for conducting journalistic investigations on human trafficking (editorial supervision)\rFernández, L. (2021)\rEconomic and Social Research Consortium (CIES)\rGender-based violence in the family environment against children and adolescents perceived as part of the LGTBI population\rHuaita Alegre, M., Chavez Granda, J., Cornejo Castellano, G. \u0026 Saravia Pinazo, M. (2019)\rJudicial Branch of Peru\rThe ratification of CEDAW as a milestone in the struggle for women's rights in Peru\rHuaita Alegre, M., \u0026 Cornejo Castellano, G. (2019).\rIus et Veritas\rEquality to build democracy: Analysis of LGTBI candidacies in the 2006–2016 electoral processes\rAlza, Carlos, et al. (2017).\rNational Jury of Elections and Observatory of Public Policies for Sexual Diversity (DISEX), PUCP\r","title":"CV","type":"page"},{"content":"","date":"13 March 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"My career began in Peru, working in academic research and development projects with people and organizations seeking to create real change. That\u0026rsquo;s where I discovered that the problem was rarely a lack of data, but rather knowing how to turn it into information and knowledge.\nThat insight motivated me to expand my professional profile: I decided to specialize in data analysis and data science because I want to build that bridge between data and the decisions that truly matter, and to help rigorous evidence guide actions aligned with my personal purpose.\nI currently work from Madrid on Vestas\u0026rsquo; global Construction Quality team. Here in Vestas, the world leader in sustainable energy, I collaborate with colleagues across five continents to create data-driven tools that enable continuous monitoring and improvement of our wind turbine quality, contributing to accelerating the energy transition.\nSo, I have two non-negotiables: having tangible impact in my role and working for an organization whose mission addresses problems that truly matter.\nMy experience includes:\nData analysis in operational and development contexts Real-time data visualization (Power BI) and static visualizations (R, Python) Communicating complex insights in language accessible to both technical and non-technical audiences, because I firmly believe that speaking plainly is a way to democratize knowledge Quantitative and qualitative research R, Python, Power BI, SQL, Excel, and QGIS ","date":"4 February 2025","externalUrl":null,"permalink":"/en/acerca-de/","section":"","summary":"My career began in Peru, working in academic research and development projects with people and organizations seeking to create real change. That’s where I discovered that the problem was rarely a lack of data, but rather knowing how to turn it into information and knowledge.\n","title":"About","type":"page"},{"content":"","externalUrl":null,"permalink":"/en/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/en/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/en/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/en/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]